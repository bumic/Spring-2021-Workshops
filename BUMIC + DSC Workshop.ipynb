{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BUMIC + DSC Workshop.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JtOsGjuyY_ik"},"source":["# Using a Pretrained Model"]},{"cell_type":"markdown","metadata":{"id":"wEejlpv9f7VJ"},"source":["## Choose a model"]},{"cell_type":"markdown","metadata":{"id":"-Y9DYPtRdhFp"},"source":["Look at pretrained models from https://pytorch.org/hub/\n","\n","ResNet is a popular computer vision model: https://pytorch.org/hub/pytorch_vision_resnet/\n","* Much deeper than earlier models\n","* Uses residual layers (skip connections)"]},{"cell_type":"markdown","metadata":{"id":"VhMkH0VWa0zd"},"source":["Get the model from PyTorch Hub:"]},{"cell_type":"code","metadata":{"id":"anp4n9SLa0AU"},"source":["import torch\n","\n","model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LUkY0eb9nrlA"},"source":["Take a look at the layers in the model:"]},{"cell_type":"code","metadata":{"id":"cCR62Pu9nhE8"},"source":["model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_6YhJLyef-Tp"},"source":["## Upload and format an input image"]},{"cell_type":"markdown","metadata":{"id":"eHAA3nX4d9Lu"},"source":["Upload a file directly into Google Colab (left menu)."]},{"cell_type":"markdown","metadata":{"id":"5cI4AqP-Zfzk"},"source":["See the files that have been uploaded:"]},{"cell_type":"code","metadata":{"id":"A2s9GIzRZNE3"},"source":["# change the path to look around your drive and find your image\n","\n","!ls /content/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p-RJ807WZly-"},"source":["image_path = '/content/IMAGENAME.jpeg' # example"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Su_fDYfGZqFZ"},"source":["View the image:"]},{"cell_type":"code","metadata":{"id":"FGJXTccIZrb0"},"source":["from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","sample_img = Image.open(image_path)\n","plt.imshow(sample_img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z40fWw8laoPH"},"source":["Resnet expects input images to be in a certain format:\n","* Specific dimensions: 256 x 256\n","* Specific distribution of input values\n","* These are provided on the model description page (from above)\n","\n","Transform the image into the format expected by ResNet:"]},{"cell_type":"code","metadata":{"id":"YJRJCqRYatQH"},"source":["from torchvision import transforms\n","\n","# this transform is the one that happened to be used by the people who trained \n","# ResNet. all inputs need to use this transform.\n","data_transform = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","  ])\n","\n","# transform our image\n","transformed_img = data_transform(sample_img)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z7mUtu4bep4c"},"source":["Now, the image is in the correct format for ResNet."]},{"cell_type":"markdown","metadata":{"id":"fr9IbplkbHma"},"source":["## Run the model on the image"]},{"cell_type":"code","metadata":{"id":"XVQSxmXPbMQP"},"source":["model.eval()\n","\n","with torch.no_grad():\n","  input = transformed_img.unsqueeze(0)  # 'unsqueeze' adds an extra dimension\n","  output = model(input)  # run the model\n","\n","# the model outputs 1000 different probabilities for each of 1000 classes.\n","# find the class with the the highest probability.\n","predicted_class = torch.argmax(output[0]).item()\n","\n","print(predicted_class)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ytc68qHnfdCd"},"source":["See which category this id corresponds to: https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/"]},{"cell_type":"markdown","metadata":{"id":"5byIbNmInye8"},"source":["## Additional Resources\n","\n","* [ResNet paper](https://arxiv.org/pdf/1512.03385.pdf)\n","* [AlexNet paper](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)"]}]}